{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krishna-singh-rajput/AI-AND-ML/blob/main/MLP_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Reproducibility (same result each run)\n",
        "# ---------------------------\n",
        "np.random.seed(42)\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Load dataset (digits: 1797 samples, each is 8x8 image)\n",
        "# ---------------------------\n",
        "digits = load_digits()\n",
        "X = digits.data          # shape: (N, 64) already flattened 8x8 -> 64\n",
        "y = digits.target        # shape: (N,)\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Train-test split (test set for honest evaluation)\n",
        "# ---------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# eg: 10000 samples fro training, 2000 images for testing... 5 classes: 500-700. 10 fold stratified sampling\n",
        "# i trained on 9 folds ... tested on 1 fold : 600 samples for each class\n",
        "# class 1: 10%, class 2: 20%, class 3: 30%, class 4: 40%, class 5: 50%\n",
        "\n"
      ],
      "metadata": {
        "id": "yhs1kmD0tctk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "c020160c-da84-49dd-c3fa-6ed0eef43006"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3615570669.py, line 26)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3615570669.py\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    eg: 10000 samples fro training, 2000 images for testing... 5 classes: 500-700. 10 fold stratified sampling\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUJ8KSEJ0Nkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Feature scaling (VERY important for MLP)\n",
        "# ---------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # fit only on train (no leakage)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) One-hot encoding for multiclass cross-entropy\n",
        "# ---------------------------\n",
        "num_classes = 10\n",
        "Y_train = np.eye(num_classes)[y_train]   # shape: (N_train, 10)\n",
        "Y_test  = np.eye(num_classes)[y_test]\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Helper functions (activations + stable softmax)\n",
        "# ---------------------------\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_grad(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def softmax(z):\n",
        "    # stability trick: subtract max per row (prevents overflow)\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(y_true_onehot, y_pred_prob):\n",
        "    # avoid log(0)\n",
        "    eps = 1e-12\n",
        "    y_pred_prob = np.clip(y_pred_prob, eps, 1 - eps)\n",
        "    # CE = - sum y*log(p) / N\n",
        "    return -np.mean(np.sum(y_true_onehot * np.log(y_pred_prob), axis=1))\n",
        "\n",
        "# ---------------------------\n",
        "# 6) MLP architecture (64 -> hidden -> 10)\n",
        "# ---------------------------\n",
        "input_dim  = 64\n",
        "hidden_dim = 64\n",
        "output_dim = 10\n",
        "\n",
        "# He initialization (good for ReLU)\n",
        "W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2 / input_dim)\n",
        "b1 = np.zeros((1, hidden_dim))\n",
        "W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2 / hidden_dim)\n",
        "b2 = np.zeros((1, output_dim))\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Training hyperparameters\n",
        "# ---------------------------\n",
        "lr = 0.1          # learning rate\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "# ---------------------------\n",
        "# 8) Training loop (forward -> loss -> backward -> update)\n",
        "# ---------------------------\n",
        "N = X_train.shape[0]\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # shuffle data each epoch (SGD works better)\n",
        "    idx = np.random.permutation(N)\n",
        "    X_train_shuff = X_train[idx]\n",
        "    Y_train_shuff = Y_train[idx]\n",
        "\n",
        "    for start in range(0, N, batch_size):\n",
        "        end = start + batch_size\n",
        "        Xb = X_train_shuff[start:end]   # mini-batch features\n",
        "        Yb = Y_train_shuff[start:end]   # mini-batch labels (one-hot)\n",
        "\n",
        "        # ---- Forward pass ----\n",
        "        Z1 = Xb @ W1 + b1               # (B,64)@(64,H)->(B,H)\n",
        "        A1 = relu(Z1)                   # non-linearity\n",
        "        Z2 = A1 @ W2 + b2               # (B,H)@(H,10)->(B,10)\n",
        "        P  = softmax(Z2)                # class probabilities\n",
        "\n",
        "        # ---- Loss (for monitoring) ----\n",
        "        # (we won't print per batch; too noisy)\n",
        "\n",
        "        # ---- Backward pass ----\n",
        "        # For softmax + cross-entropy: dZ2 = (P - Y) / B\n",
        "        B = Xb.shape[0]\n",
        "        dZ2 = (P - Yb) / B              # (B,10)\n",
        "\n",
        "        dW2 = A1.T @ dZ2                # (H,B)@(B,10)->(H,10)\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "        dA1 = dZ2 @ W2.T                # (B,10)@(10,H)->(B,H)\n",
        "        dZ1 = dA1 * relu_grad(Z1)       # chain rule through ReLU\n",
        "\n",
        "        dW1 = Xb.T @ dZ1                # (64,B)@(B,H)->(64,H)\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "        # ---- Parameter update (SGD) ----\n",
        "        W2 -= lr * dW2\n",
        "        b2 -= lr * db2\n",
        "        W1 -= lr * dW1\n",
        "        b1 -= lr * db1\n",
        "\n",
        "    # ---- End of epoch: evaluate quickly ----\n",
        "    Z1t = X_train @ W1 + b1\n",
        "    A1t = relu(Z1t)\n",
        "    Z2t = A1t @ W2 + b2\n",
        "    Pt  = softmax(Z2t)\n",
        "    train_loss = cross_entropy(Y_train, Pt)\n",
        "    train_pred = np.argmax(Pt, axis=1)\n",
        "    train_acc  = accuracy_score(y_train, train_pred)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 9) Final Test Evaluation\n",
        "# ---------------------------\n",
        "Z1 = X_test @ W1 + b1\n",
        "A1 = relu(Z1)\n",
        "Z2 = A1 @ W2 + b2\n",
        "P  = softmax(Z2)\n",
        "y_pred = np.argmax(P, axis=1)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPvY0r-DlDgy",
        "outputId": "97b97ef8-e542-4300-d503-06e975b8316f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 0.7491 | Train Acc: 0.8219\n",
            "Epoch 05 | Train Loss: 0.1997 | Train Acc: 0.9631\n",
            "Epoch 10 | Train Loss: 0.1037 | Train Acc: 0.9847\n",
            "Epoch 15 | Train Loss: 0.0672 | Train Acc: 0.9916\n",
            "Epoch 20 | Train Loss: 0.0479 | Train Acc: 0.9951\n",
            "Epoch 25 | Train Loss: 0.0364 | Train Acc: 0.9972\n",
            "Epoch 30 | Train Loss: 0.0288 | Train Acc: 0.9986\n",
            "Epoch 35 | Train Loss: 0.0235 | Train Acc: 0.9993\n",
            "Epoch 40 | Train Loss: 0.0197 | Train Acc: 0.9993\n",
            "Epoch 45 | Train Loss: 0.0169 | Train Acc: 1.0000\n",
            "Epoch 50 | Train Loss: 0.0147 | Train Acc: 1.0000\n",
            "\n",
            "Test Accuracy: 0.9611111111111111\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.99        36\n",
            "           1       0.88      0.97      0.92        36\n",
            "           2       0.97      1.00      0.99        35\n",
            "           3       1.00      0.97      0.99        37\n",
            "           4       0.92      0.97      0.95        36\n",
            "           5       1.00      1.00      1.00        37\n",
            "           6       1.00      0.97      0.99        36\n",
            "           7       0.95      0.97      0.96        36\n",
            "           8       0.97      0.80      0.88        35\n",
            "           9       0.95      0.97      0.96        36\n",
            "\n",
            "    accuracy                           0.96       360\n",
            "   macro avg       0.96      0.96      0.96       360\n",
            "weighted avg       0.96      0.96      0.96       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# -------------------------\n",
        "# 1) Reproducibility (same results)\n",
        "# -------------------------\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# -------------------------\n",
        "# 2) Load digits dataset\n",
        "# digits.images shape: (N, 8, 8)\n",
        "# -------------------------\n",
        "digits = load_digits()\n",
        "X_img = digits.images          # (N, 8, 8)\n",
        "y = digits.target              # (N,)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Train-test split\n",
        "# -------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_img, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Normalize inputs\n",
        "# digits pixel values are 0..16\n",
        "# CNN likes float + scaled values\n",
        "# -------------------------\n",
        "X_train = X_train.astype(np.float32) / 16.0\n",
        "X_test  = X_test.astype(np.float32) / 16.0\n",
        "\n",
        "# -------------------------\n",
        "# 5) Add channel dimension\n",
        "# CNN expects: (batch, channels, height, width)\n",
        "# Here grayscale => channels=1\n",
        "# -------------------------\n",
        "X_train = X_train[:, None, :, :]   # (N, 1, 8, 8)\n",
        "X_test  = X_test[:, None, :, :]    # (N, 1, 8, 8)\n",
        "\n",
        "# -------------------------\n",
        "# 6) Convert to torch tensors\n",
        "# -------------------------\n",
        "X_train_t = torch.tensor(X_train)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "X_test_t  = torch.tensor(X_test)\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# -------------------------\n",
        "# 7) DataLoader (mini-batches)\n",
        "# -------------------------\n",
        "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=128, shuffle=False)\n",
        "\n",
        "# -------------------------\n",
        "# 8) CNN Model\n",
        "# Conv -> ReLU -> Pool -> Conv -> ReLU -> Pool -> Flatten -> FC -> Output(10)\n",
        "# -------------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.pool  = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # After pooling twice:\n",
        "        # Input 8x8 -> pool -> 4x4 -> pool -> 2x2\n",
        "        # Channels after conv2 = 32\n",
        "        # So flattened size = 32 * 2 * 2 = 128\n",
        "        self.fc1 = nn.Linear(32 * 2 * 2, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # (B,16,8,8) -> (B,16,4,4)\n",
        "        x = self.pool(self.relu(self.conv2(x)))  # (B,32,4,4) -> (B,32,2,2)\n",
        "        x = x.view(x.size(0), -1)                # flatten -> (B,128)\n",
        "        x = self.relu(self.fc1(x))               # (B,64)\n",
        "        x = self.fc2(x)                          # logits (B,10)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()\n",
        "\n",
        "# -------------------------\n",
        "# 9) Loss + Optimizer\n",
        "# CrossEntropyLoss = Softmax + CE combined (numerically stable)\n",
        "# -------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# -------------------------\n",
        "# 10) Train loop\n",
        "# -------------------------\n",
        "epochs = 20\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()          # (why) clear old gradients\n",
        "        logits = model(xb)             # forward\n",
        "        loss = criterion(logits, yb)   # compute loss\n",
        "        loss.backward()                # backprop\n",
        "        optimizer.step()               # update weights\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # -------------------------\n",
        "    # 11) Quick evaluation each epoch\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():  # (why) no gradients needed during eval\n",
        "        for xb, yb in test_loader:\n",
        "            logits = model(xb)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    if epoch == 1 or epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch:02d} | Train Loss: {total_loss/len(train_loader):.4f} | Test Acc: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "Anjvzf8-323n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}